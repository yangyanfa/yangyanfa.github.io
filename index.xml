<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YYF</title>
    <link>https://yangyanfa.github.io/</link>
    <description>Recent content on YYF</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2019 21:12:37 +0800</lastBuildDate>
    
	<atom:link href="https://yangyanfa.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>9.6</title>
      <link>https://yangyanfa.github.io/post/9.6/</link>
      <pubDate>Fri, 06 Sep 2019 21:12:37 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/9.6/</guid>
      <description> idea常用快捷键总结（不断更新）  代码提示
alt+/ 代码上下行移动
alt+shift+上、下 删除某行代码
ctr+Y 复制某行代码
ctr+D 自动收尾
ctr+shift+enter 格式化代码
ctr+alt+L 根据表达式计算结果，自动生成对应的变量及类型名
ctr+alt+V 生成array for代码块
itar  </description>
    </item>
    
    <item>
      <title>9.2</title>
      <link>https://yangyanfa.github.io/post/9.2/</link>
      <pubDate>Mon, 02 Sep 2019 14:49:38 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/9.2/</guid>
      <description> Markdown常用语法总结  各级标题
H3 H4 换行
在某行的最后敲至少两个空格，然后回车
 强调
加粗：选中要强调的内容ctr+B
*斜体*：选中要强调的内容ctr+I
 分割线
 代码展示
system.out.println();
 链接
百度
 插入图片
   调整图片的大小和位置   </description>
    </item>
    
    <item>
      <title>8.26</title>
      <link>https://yangyanfa.github.io/post/8.26/</link>
      <pubDate>Mon, 26 Aug 2019 20:38:36 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/8.26/</guid>
      <description>人工智能、机器学习与深度学习的关系 人工智能分为：强人工智能和应用人工智能。
机器学习是人工智能的主要方法，而深度学习是机器学习的分支。
强人工智能是最终希望实现的目标，而应用人工智能是现在主要的研究领域，主要包含以下几个方面：
1.计算机视觉
计算机视觉又包括图像分类、物体检测、语义分割、视频分析等
2.语音识别
主要包含声纹识别、语音识别等
3.自然语言处理
主要包括机器翻译、阅读理解、自动摘要、文本分类、中文分词等
4.推荐系统和专家系统
帮助机器实现个性化的推荐系统，解决特定问题的专家系统等等</description>
    </item>
    
    <item>
      <title>8.25</title>
      <link>https://yangyanfa.github.io/post/8.25/</link>
      <pubDate>Sun, 25 Aug 2019 19:59:30 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/8.25/</guid>
      <description>Windows 常用快捷键总结（以后将不断更新） win+ctrl+D -&amp;gt; 新建一个桌面，可以在任务视图里切换（win+tab）
ctrl+shift+esc -&amp;gt; 打开任务窗口
win+shift+s -&amp;gt; 截屏</description>
    </item>
    
    <item>
      <title>8.24</title>
      <link>https://yangyanfa.github.io/post/8.24/</link>
      <pubDate>Sat, 24 Aug 2019 21:13:19 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/8.24/</guid>
      <description>如何将批量win的cmd命令写成自动执行的脚本文件？ 1.新建一个TXT文件，将要执行的cmd命令一条一条地写上，条与条之间换行。
2.全部写完之后，将TXT文件另存为.bat文件，注意文件保存的路径与cmd命令所执行路径的关系。
Java的基本语法运用 1.键盘输入：
Scanner sc = new Scanner(System.in);
int n = sc.nextInt();
或者 char c = sc.nextChar();
或者 float f = sc.nextFloat();
2.字符串定位函数及分割函数：
String s = “56+12*4-9”；
或者String s = new String(&amp;ldquo;9+3+4*3&amp;rdquo;);
s.charAt(0); //字符串s中第一个位置的元素
String strarray = s.split(&amp;rdquo;\+&amp;ldquo;);
//在Java中，不管是String.split()，还是正则表达式，有一些特殊字符需要转义，
这些字符是
( [ { / ^ - $ ¦ } ] ) ? * + .
转义方法为字符前面加上“\ \”,这样在split、replaceAll时就不会报错了；
不过要注意，String.contains()方法不需要转义。
3.初始化一维数组
int[] a = new int[100];
int[] a = new int{1,2,3,4};</description>
    </item>
    
    <item>
      <title>大数据面试</title>
      <link>https://yangyanfa.github.io/post/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95/</link>
      <pubDate>Sat, 24 Aug 2019 19:14:26 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95/</guid>
      <description>大数据面试关键过程 Hadoop1.0内核主要由HDFS和MapReduce两个系统组成，其中MapReduce是一个离线处理框架，由编程模型（新旧API）、运行时环境（JobTracker和TaskTracker）和数据处理引擎（MapTask和ReduceTask）三部分组成。
Hadoop2.0内核主要由HDFS、MapReduce和YARN三个系统组成，其中YARN是一个资源管理系统，负责集群资源管理和调度，MapReduce则是运行在YARN上的离线处理框架，它与Hadoop 1.0中的MapReduce在编程模型（新旧API）和数据处理引擎（MapTask和ReduceTask）两个方面是相同的。唯一不同的是运行时环境。
Hadoop1的架构是：多个Application &amp;ndash;&amp;gt; JobTracker &amp;ndash;&amp;gt; 多个TaskTracker。
hadoop2的架构是：多个application &amp;ndash;&amp;gt; ResourceManager &amp;ndash;&amp;gt; 多个NodeManager &amp;ndash;&amp;gt; 多个application Master &amp;ndash;&amp;gt; 多个container
HDFS读写数据的过程
读：
1、跟namenode通信查询元数据，找到文件块所在的datanode服务器
2、挑选一台datanode(就近原则，然后随机)服务器，请求建立socket流
3、datanode开始发送数据(从磁盘里面读取数据放入流，以packet为单位来做校验)
4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件
写：
1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在
2、namenode返回是否可以上传
3、client请求第一个 block该传输到哪些datanode服务器上
4、namenode返回3个datanode服务器ABC
5、client请求3台dn中的一台A上传数据(本质上是一个RPC调用，建立pipeline)，A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端
6、client开始往A上传第一个block(先从磁盘读取数据放到一个本地内存缓存)，以packet为单位，A收到一个packet就会传给B，B传给C;A每传一个packet会放入一个应答队列等待应答
7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。
hadoop的shuffle过程
一、Map端的shuffle
Map端会处理输入数据并产生中间结果，这个中间结果会写到本地磁盘，而不是HDFS。每个Map的输出会先写到内存缓冲区中，当写入的数据达到设定的阈值时，系统将会启动一个线程将缓冲区的数据写到磁盘，这个过程叫做spill。 在spill写入之前，会先进行二次排序，首先根据数据所属的partition进行排序，然后每个partition中的数据再按key来排序。partition的目是将记录划分到不同的Reducer上去，以期望能够达到负载均衡，以后的Reducer就会根据partition来读取自己对应的数据。接着运行combiner(如果设置了的话)，combiner的本质也是一个Reducer，其目的是对将要写入到磁盘上的文件先进行一次处理，这样，写入到磁盘的数据量就会减少。最后将数据写到本地磁盘产生spill文件(spill文件保存在{mapred.local.dir}指定的目录中，Map任务结束后就会被删除)。 最后，每个Map任务可能产生多个spill文件，在每个Map任务完成前，会通过多路归并算法将这些spill文件归并成一个文件。至此，Map的shuffle过程就结束了。
二、Reduce端的shuffle
Reduce端的shuffle主要包括三个阶段，copy、sort(merge)和reduce。
首先要将Map端产生的输出文件拷贝到Reduce端，但每个Reducer如何知道自己应该处理哪些数据呢?因为Map端进行partition的时候，实际上就相当于指定了每个Reducer要处理的数据(partition就对应了Reducer)，所以Reducer在拷贝数据的时候只需拷贝与自己对应的partition中的数据即可。每个Reducer会处理一个或者多个partition，但需要先将自己对应的partition中的数据从每个Map的输出结果中拷贝过来。 接下来就是sort阶段，也成为merge阶段，因为这个阶段的主要工作是执行了归并排序。从Map端拷贝到Reduce端的数据都是有序的，所以很适合归并排序。最终在Reduce端生成一个较大的文件作为Reduce的输入。 最后就是Reduce过程了，在这个过程中产生了最终的输出结果，并将其写到HDFS上。
Reducer如何知道自己应该处理哪些数据呢？
因为Map端进行partition的时候，实际上就相当于指定了每个Reducer要处理的数据(partition就对应了Reducer)，所以Reducer在拷贝数据的时候只需拷贝与自己对应的partition中的数据即可。每个Reducer会处理一个或者多个partition。 MapReduce为每个数据分片（split）生成一个Map任务，对每条记录（RR）调用一次map( )方法。若一个Map任务处理的数据分片有n条记录，则在该Map任务中会调用n次map( )方法。map( )方法对输入的进行处理，产生一系列新的键/值对[]输出。 在进入Reducer处理前，必须等所有的Map任务完成，因此在进入Reducer前需要一个同步障。这个阶段也负责对map输出的数据进行收集和整理，以便Reduce节点可以完全基于本节点上的数据计算。因此，同步障中对Map输出数据的一个重要处理就是按键值的数据分区（Partitioner）。 Partitioner的作用就是对Map输出的中间结果按key进行分组，同一组的数据交给同一个Reduce节点处理，以避免Reduce节点在处理中访问其他Reduce节点。HadoopMapReduce框架中自带了一个HashPartitioner类，默认情况下，HashPartitioner对Map输出的中间结果中的key取hash值，并按Reduce节点数取模来确定分配到哪一个Reduce节点。相同key值的中间结果一定送到同一个Reduce节点。 Reducer是对Partitioner分区后送来的一组[]进行合并处理的抽象类，其中重要的就是reduce( )方法。MapReduce为Reduce节点上相同key的一组value值（即）调用一次reduce( )方法，对进行整理或处理，产生最终的结果[]。
Cluster Manager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器
Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。
Driver： 运行Application 的main()函数
Executor：执行器，是为某个Application运行在worker node上的一个进程
spark运行流程图如下：
1.构建Spark Application的运行环境，启动SparkContext
2.SparkContext向资源管理器（可以是Standalone，Mesos，Yarn）申请运行Executor资源，并启动StandaloneExecutorbackend，
3.Executor向SparkContext申请Task
4.SparkContext将应用程序分发给Executor</description>
    </item>
    
    <item>
      <title>小中大</title>
      <link>https://yangyanfa.github.io/post/%E5%B0%8F%E4%B8%AD%E5%A4%A7/</link>
      <pubDate>Sat, 24 Aug 2019 10:16:03 +0800</pubDate>
      
      <guid>https://yangyanfa.github.io/post/%E5%B0%8F%E4%B8%AD%E5%A4%A7/</guid>
      <description>小中大 题目说明： 找出一组有序数列中的最大值、中位数、和最小值。
Java代码： package 小中大;
import java.util.Arrays;
import java.util.Scanner;
public class PaiXu {
public static void main(String[] args) { int n = 0; @SuppressWarnings(&amp;quot;resource&amp;quot;) Scanner sc = new Scanner(System.in); n = sc.nextInt(); int[] a = new int[n]; for (int i = 0; i &amp;lt; a.length; i++) { a[i] = sc.nextInt(); } Arrays.sort(a); if(n%2 != 0) { System.out.println(a[n-1] + &amp;quot; &amp;quot; + a[(n-1)/2] + &amp;quot; &amp;quot; + a[0]); } if(n%2 == 0) { float m = (float)(a[n/2] + a[(n-2)/2])/2; String result = String.</description>
    </item>
    
  </channel>
</rss>